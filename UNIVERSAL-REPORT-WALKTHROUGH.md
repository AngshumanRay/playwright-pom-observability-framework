# ğŸ“Š Universal Test Report â€” Customer Walkthrough

> **Document purpose:** Explain every section of the Universal Test Report to customers, stakeholders, and team members.
>
> **Report location:** `Reports/universal-report/index.html`
>
> **Generated by:** `reporters/UniversalReporter.ts` â€” a single, self-contained Playwright reporter with zero external dependencies.

---

## Table of Contents

1. [Report Overview](#1-report-overview)
2. [Header Bar](#2-header-bar)
3. [Tab 1 â€” ğŸ“ˆ Dashboard](#3-tab-1--dashboard)
4. [Tab 2 â€” ğŸ§ª Tests](#4-tab-2--tests)
5. [Tab 3 â€” âš¡ Performance](#5-tab-3--performance)
6. [Tab 4 â€” ğŸ”­ Observability](#6-tab-4--observability)
7. [Tab 5 â€” ğŸ”’ Security](#7-tab-5--security)
8. [Tab 6 â€” â™¿ Accessibility](#8-tab-6--accessibility)
9. [Tab 7 â€” ğŸŒ Browsers](#9-tab-7--browsers)
10. [Scoring System](#10-scoring-system)
11. [Interactive Features](#11-interactive-features)
12. [How to Run](#12-how-to-run)
13. [FAQ](#13-faq)

---

## 1. Report Overview

The Universal Test Report is a **single HTML file** that opens in any browser. It provides a comprehensive view of your test suite across **7 interactive tabs**:

| Tab | Icon | Purpose | Primary Audience |
|-----|------|---------|------------------|
| Dashboard | ğŸ“ˆ | Executive summary â€” KPIs, charts, overall health | Managers, stakeholders |
| Tests | ğŸ§ª | Individual test details â€” steps, errors, screenshots | QA engineers, developers |
| Performance | âš¡ | Speed analysis â€” 3D scatter, box plots, histograms | Performance engineers |
| Observability | ğŸ”­ | Network monitoring â€” requests, response times, errors | DevOps, SREs, backend teams |
| Security | ğŸ”’ | Risk assessment â€” findings, posture radar, severity | Security leads, compliance |
| Accessibility | â™¿ | WCAG compliance â€” violations, severity, affected rules | Accessibility specialists, UX |
| Browsers | ğŸŒ | Cross-browser comparison â€” radar, tables, pass rates | QA leads, browser compat teams |

**Key characteristics:**
- âœ… Single self-contained HTML file (no server needed)
- âœ… Works offline after first load (charts use CDN on first open)
- âœ… Responsive â€” works on desktop, tablet, and mobile
- âœ… Print-friendly â€” Ctrl+P produces clean output
- âœ… Dark theme with high-contrast design

---

## 2. Header Bar

The purple gradient header is always visible at the top and provides at-a-glance metadata.

| Field | What It Shows | Why It Matters |
|-------|---------------|----------------|
| **Generated** | Exact date and time the report was created | Ensures you're looking at the latest run |
| **Duration** | Total wall-clock time for the entire test suite | Tracks CI/CD pipeline efficiency |
| **Tests** | Total number of test cases executed | Confirms expected test count |
| **Pass Rate** | Percentage of tests that passed | Quick go/no-go indicator |
| **Platform** | OS, architecture, and kernel version | Important for reproducing issues |
| **Playwright** | Playwright version used | Ensures version consistency across environments |

---

## 3. Tab 1 â€” ğŸ“ˆ Dashboard

**Purpose:** Give stakeholders a single-screen health check of the entire test suite.

### KPI Cards (8 cards)

| Card | What It Measures | Color Logic |
|------|------------------|-------------|
| **Benchmark** | Overall quality score (0â€“100) with tier badge | Composite of speed + reliability + accessibility |
| **Pass Rate** | Percentage of tests that passed | ğŸŸ¢ â‰¥ 90% Â· ğŸŸ¡ â‰¥ 70% Â· ğŸ”´ < 70% |
| **Total Tests** | Test count with pass/fail breakdown | â€” |
| **Median Duration** | Typical test runtime + P95 worst-case | â€” |
| **Network** | Total HTTP requests + failure count | ğŸŸ¢ 0 failures Â· ğŸ”´ any failures |
| **Observability** | Combined network health + error freedom score | ğŸŸ¢ â‰¥ 80 Â· ğŸŸ¡ â‰¥ 50 Â· ğŸ”´ < 50 |
| **Security** | Risk assessment score + risk level | ğŸŸ¢ â‰¥ 80 Â· ğŸŸ¡ â‰¥ 50 Â· ğŸ”´ < 50 |
| **A11y** | Accessibility compliance score + violation count | ğŸŸ¢ â‰¥ 90 Â· ğŸŸ¡ â‰¥ 60 Â· ğŸ”´ < 60 |

### Charts (4 charts in 2Ã—2 grid)

| Chart | What It Shows | How to Read It |
|-------|---------------|----------------|
| **Test Results Donut** | Visual split: Passed (green) / Failed (red) / Skipped (amber) / Timed Out (orange) | Mostly green = healthy suite |
| **Duration by Test (Top 20)** | Horizontal bar â€” longest tests at top, colored by status | Red bars = slow AND failing â€” top priority |
| **Results by File** | Stacked bar â€” pass/fail count per spec file | Identifies problematic test files |
| **Benchmark Tier Pie** | Distribution across quality tiers (Elite â†’ Critical) | Ideally dominated by Elite/Strong |

**ğŸ’¬ Customer talking point:** *"This dashboard gives you a single-screen health check. If the Benchmark is green and Elite â€” your test suite is fast, reliable, and accessible. You're ready to ship."*

---

## 4. Tab 2 â€” ğŸ§ª Tests

**Purpose:** Drill into individual test results for debugging and review.

### Controls

| Control | Function |
|---------|----------|
| **ğŸ” Search box** | Type any keyword to filter by test name, file path, or browser |
| **All** button | Show all tests (default) |
| **âœ… Passed** button | Show only passing tests |
| **âŒ Failed** button | Show only failed tests |
| **â­ Skipped** button | Show only skipped tests |

### Each Test Card

Every test is displayed as an expandable card. Click to expand and see full details.

**Card header (always visible):**

| Element | Description |
|---------|-------------|
| Left border color | ğŸŸ¢ Green = passed Â· ğŸ”´ Red = failed/timed out Â· ğŸŸ¡ Amber = skipped |
| Test name | Full test title |
| File path | Source file location (gray text) |
| Duration | How long the test took |
| Browser | Which browser ran this test |
| Status badge | PASSED / FAILED / TIMED OUT / SKIPPED |
| Tier badge | Elite / Strong / Stable / Watch / Critical |

**Expanded details (click to reveal):**

| Section | What It Shows | When It Appears |
|---------|---------------|-----------------|
| **Error block** | Full error message + stack trace in red box | Failed tests only |
| **Steps** | Tree-structured step-by-step execution with timings. Failed steps highlighted with âŒ | Always |
| **Screenshots** | Embedded failure screenshots. Click to zoom full-screen | When `screenshot: 'only-on-failure'` is configured |
| **Trace link** | Link to Playwright trace viewer | When `trace: 'retain-on-failure'` is configured |
| **ğŸ”­ Observability Metrics** | Per-test network data (see below) | Always |
| **â™¿ Accessibility** | Per-test WCAG violations | When violations are found |

**ğŸ”­ Observability Metrics inset (shown for every test):**

| Metric | Description | Color |
|--------|-------------|-------|
| Requests | Total HTTP requests made during this test | â€” |
| Failures | Network-level failures (DNS, TLS, connection errors) | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| HTTP Errors | Responses with 4xx/5xx status codes | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| Avg Response | Mean API response time in milliseconds | â€” |
| P95 | 95th percentile response time | â€” |
| Console Errors | Number of `console.error()` messages captured | ğŸŸ¢ 0 Â· ğŸŸ¡ > 0 |
| Page Errors | Number of unhandled JavaScript exceptions | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |

If console errors or page errors were captured, they are displayed in scrollable log blocks beneath the metrics.

**â™¿ Accessibility inset (only when violations found):**

Lists each violation with:
- Severity badge (Critical / Serious / Moderate / Minor)
- Rule ID (e.g., `image-alt`, `button-name`)
- Description of the issue
- Number of affected DOM nodes

**ğŸ’¬ Customer talking point:** *"Click any test to drill down. For failed tests, you get the exact error, the step that failed, a screenshot of the page at failure time, and full network telemetry â€” everything needed to diagnose the issue without re-running."*

---

## 5. Tab 3 â€” âš¡ Performance

**Purpose:** Analyze test execution speed, identify bottlenecks, and track suite efficiency.

### KPI Cards (6)

| Card | Meaning |
|------|---------|
| **Avg Duration** | Mean test execution time |
| **Median** | 50th percentile â€” more representative than average (not skewed by outliers) |
| **P95** | 95th percentile â€” "worst reasonable case" (95% of tests are faster than this) |
| **P99** | 99th percentile â€” extreme tail latency |
| **Throughput** | Tests completed per minute â€” higher is better for CI/CD |
| **Benchmark** | Overall quality score with tier |

### Charts (4)

| Chart | What It Shows | How to Interpret |
|-------|---------------|------------------|
| **3D Benchmark Cloud** | Each dot = one test. **X** = duration, **Y** = score, **Z** = retry count. Drag to rotate. Color = pass/fail. Dot size = error count. | Dots clustered **top-left** = fast + high-scoring (ideal). Dots **bottom-right** = slow + unreliable (investigate). |
| **Duration Box Plot** | One box per browser showing median (line), quartiles (box), and outliers (dots) | If one browser's box is significantly higher, that browser is slower |
| **Top 10 Slowest** | Horizontal bar â€” the 10 slowest tests, colored by tier | These are your optimization candidates |
| **Duration Histogram** | Distribution of test durations | Bell curve = consistent. Long tail = outlier tests |

### Performance Data Table

Full sortable table with columns:

| Column | Description |
|--------|-------------|
| # | Rank by duration (slowest first) |
| Test | Test name |
| Browser | Browser used |
| Duration | Execution time |
| Requests | Network requests made |
| Avg Resp | Average API response time |
| Score | Benchmark score |
| Tier | Quality tier badge |
| Status | Pass/fail badge |

**ğŸ’¬ Customer talking point:** *"The 3D cloud is your test health compass â€” ideally all dots are clustered in the fast + high-score corner. The box plot reveals if a specific browser is the bottleneck. The top 10 slowest list tells you exactly where to focus optimization efforts."*

---

## 6. Tab 4 â€” ğŸ”­ Observability

**Purpose:** Monitor network behavior, API response times, and application errors during testing.

### KPI Cards (10)

| Card | What It Monitors | Color Logic |
|------|-----------------|-------------|
| **Total Requests** | Every HTTP request made across all tests | â€” |
| **Req Failures** | Network-level failures + failure rate % | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **HTTP Errors** | Responses with 4xx/5xx status codes | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **Avg Response** | Mean API response time across all tests | ğŸŸ¢ < 300ms Â· ğŸŸ¡ < 800ms Â· ğŸ”´ â‰¥ 800ms |
| **P95 Response** | 95th percentile response time | â€” |
| **Max Response** | Slowest single API response observed | â€” |
| **Console Errors** | Total `console.error()` messages captured | ğŸŸ¢ 0 Â· ğŸŸ¡ > 0 |
| **Page Errors** | Unhandled JavaScript exceptions (`window.onerror`) | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **Network Score** | Composite health: failure rate + response time + HTTP errors (0â€“100) | ğŸŸ¢ â‰¥ 80 Â· ğŸŸ¡ â‰¥ 50 Â· ğŸ”´ < 50 |
| **Observability** | Combined network + error scores (0â€“100) | ğŸŸ¢ â‰¥ 80 Â· ğŸŸ¡ â‰¥ 50 Â· ğŸ”´ < 50 |

### Charts (4)

| Chart | What It Reveals |
|-------|-----------------|
| **Network Requests per Test** | Horizontal bar â€” which tests make the most HTTP requests. Red bars = has failures, cyan = clean |
| **Response Time Distribution** | Box plot per browser â€” shows response time spread and outliers |
| **Requests vs Response Time (Bubble)** | Scatter plot â€” each bubble is a test. Size = request count. Color = status. Tests in the **top-right** have many requests AND slow responses â€” investigate those first |
| **Error Distribution (Stacked Bar)** | Per-test breakdown: Console Errors (amber), Page Errors (red), HTTP Errors (orange) |

### Error Panels

| Panel | Content |
|-------|---------|
| **Console Errors** | Lists every unique `console.error()` message captured across all tests. Shows âœ… green checkmark if none |
| **Page Errors** | Lists every unhandled JavaScript exception. Shows âœ… green checkmark if none |

### Observability Data Table

Full table with per-test columns: Requests, Failures, HTTP Errors, Avg Response, P95 Response, Console Errors, Page Errors. Red highlighting on non-zero error values.

**ğŸ’¬ Customer talking point:** *"This tab is your application's nervous system during testing. Zero failures and sub-300ms responses mean the backend is healthy. Any red numbers here warrant investigation with your backend team. The bubble chart instantly shows which tests are the heaviest network consumers."*

---

## 7. Tab 5 â€” ğŸ”’ Security

**Purpose:** Assess application reliability and stability from a security perspective during test execution.

### KPI Cards (8)

| Card | What It Assesses | Color Logic |
|------|-----------------|-------------|
| **Security Score** | Composite score (0â€“100) | ğŸŸ¢ â‰¥ 80 Â· ğŸŸ¡ â‰¥ 50 Â· ğŸ”´ < 50 |
| **Risk Level** | Low / Medium / High / Critical | ğŸŸ¢ Low Â· ğŸŸ¡ Medium Â· ğŸ”´ High/Critical |
| **Findings** | Number of issues detected | ğŸŸ¢ 0 Â· ğŸŸ¡ > 0 |
| **HTTP Errors** | 4xx/5xx responses (may indicate broken auth, missing endpoints) | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **Net Failures** | DNS/TLS/connection errors (may indicate cert or infra issues) | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **Console Errs** | Client-side errors that may leak sensitive information | ğŸŸ¢ 0 Â· ğŸŸ¡ > 0 |
| **Page Errors** | Unhandled exceptions that could expose stack traces to users | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **Pass Rate** | Functional reliability indicator | ğŸŸ¢ â‰¥ 90% Â· ğŸ”´ < 90% |

### Charts (2)

| Chart | What It Shows |
|-------|---------------|
| **Security Posture Radar** | Pentagon with 5 dimensions: Pass Rate, Network Health, Error Freedom, Accessibility, Observability. A **full pentagon** = excellent posture. **Dents** indicate weak areas that need attention |
| **Findings by Severity Pie** | Donut breakdown: Critical (red) / High (orange) / Medium (amber) / Low (blue) with total count in center |

### Security Findings

Each finding is displayed as a card with:

| Element | Description |
|---------|-------------|
| **Severity badge** | Critical (red) Â· High (orange) Â· Medium (amber) Â· Low (blue) |
| **Category** | Type of issue (see table below) |
| **Description** | Human-readable explanation |
| **Occurrence count** | How many times the issue was observed |

### Types of Findings Automatically Detected

| Finding | What Triggers It | Severity |
|---------|-----------------|----------|
| **Network Failures** | DNS resolution errors, TLS handshake failures, connection refused | High (> 5 occurrences) or Medium |
| **HTTP Errors** | Server returning 4xx/5xx status codes | High (> 10) or Medium |
| **Console Errors** | `console.error()` messages captured from the application | High (> 5 unique) or Low |
| **Unhandled Page Errors** | `window.onerror` events (uncaught JavaScript exceptions) | Always High |
| **Test Failures** | Tests that ended in failure | Critical (> 3) or High |
| **Flaky Tests** | Tests that passed only after one or more retries | Medium |
| **Slow Responses** | Average API response time > 500ms | High (> 1000ms) or Medium |
| **P95 Latency** | 95th percentile response time > 1000ms | Medium |

### Findings Table

Same finding data in sortable table format with columns: Severity, Category, Description, Count.

**ğŸ’¬ Customer talking point:** *"The security score tells you if the application is behaving predictably during testing. A score of 100 means zero network errors, zero unhandled exceptions, zero HTTP failures, and all tests passing â€” the gold standard for a release candidate. Any findings listed here should be reviewed before signing off on a release."*

---

## 8. Tab 6 â€” â™¿ Accessibility

**Purpose:** Ensure the application is usable by everyone, including people using assistive technologies.

### KPI Cards (8)

| Card | What It Measures | Color Logic |
|------|-----------------|-------------|
| **A11y Score** | 0â€“100 (penalized by violations: criticalÃ—4, seriousÃ—3, moderateÃ—2, minorÃ—1, then Ã—5) | ğŸŸ¢ â‰¥ 90 Â· ğŸŸ¡ â‰¥ 60 Â· ğŸ”´ < 60 |
| **Total Violations** | Sum of all WCAG violations found across all tests | â€” |
| **ğŸ”´ Critical** | Completely blocks access for some users | ğŸŸ¢ 0 Â· ğŸ”´ > 0 |
| **ğŸŸ  Serious** | Creates significant barriers | ğŸŸ¢ 0 Â· ğŸŸ  > 0 |
| **ğŸŸ¡ Moderate** | Usability issues but workarounds exist | â€” |
| **ğŸ”µ Minor** | Best-practice recommendations | â€” |
| **Tests Scanned** | How many tests had accessibility scanning | â€” |
| **With Issues** | How many tests had at least one violation | â€” |

### Charts (2)

| Chart | What It Shows |
|-------|---------------|
| **Violations by Severity Pie** | Donut showing Critical / Serious / Moderate / Minor distribution with total in center |
| **Top Violations List** | Ranked list of the most common violations with: occurrence count (colored by severity), rule ID, impact badge, and human-readable description |

### WCAG Rules Checked

When using the observability fixture, these accessibility rules are automatically scanned on every test:

| Rule ID | What It Checks |
|---------|----------------|
| `image-alt` | All images must have alternative text |
| `button-name` | Buttons must have discernible text for screen readers |
| `heading-order` | Heading levels must be sequential (h1 â†’ h2 â†’ h3, not h1 â†’ h3) |
| `label` | All form inputs must have associated labels |
| `landmark-one-main` | Page must have exactly one `<main>` landmark |
| `region` | All page content must be contained within landmark regions |
| `color-contrast` | Text must have sufficient color contrast against its background |
| `link-name` | Links must have discernible text (not just "click here") |

### Severity Definitions

| Severity | Meaning | Action Required |
|----------|---------|-----------------|
| **Critical** | Users with disabilities are completely blocked from accessing content | Must fix before release |
| **Serious** | Users face significant barriers; some functionality is inaccessible | Should fix before release |
| **Moderate** | Users experience difficulty but can still access content with workarounds | Fix in next sprint |
| **Minor** | Best practice recommendations; does not block access | Add to backlog |

**ğŸ’¬ Customer talking point:** *"This tab ensures your application meets WCAG accessibility standards. A score of 100 means zero violations detected across all pages tested. Critical and serious violations should be addressed before release â€” they represent real barriers for users who rely on screen readers, keyboard navigation, or other assistive technologies."*

---

## 9. Tab 7 â€” ğŸŒ Browsers

**Purpose:** Compare test results and performance across different browsers.

### Charts (2)

| Chart | What It Shows |
|-------|---------------|
| **Browser Radar** | Overlaid pentagons per browser comparing 5 dimensions: Pass Rate, Benchmark Score, Speed, Network volume, Accessibility. If one browser's shape is noticeably smaller, it has weaknesses in those areas |
| **Pass Rate vs Benchmark (Grouped Bar)** | Side-by-side bars per browser: green = pass rate %, purple = benchmark score |

### Comparison Table

| Column | What It Shows |
|--------|---------------|
| **Browser** | Name (chromium, firefox, webkit, edge) |
| **Score** | Benchmark score for that browser |
| **Tier** | Quality tier badge (Elite â†’ Critical) |
| **Tests** | Count of tests run on this browser |
| **Pass Rate** | Percentage that passed on this browser |
| **Avg Duration** | Mean test execution time |
| **P95** | 95th percentile duration (worst-case timing) |
| **Requests** | Total network requests made on this browser |
| **Avg Resp** | Mean API response time on this browser |
| **A11y** | Number of accessibility violations on this browser |

**ğŸ’¬ Customer talking point:** *"This tab answers the question: 'Does our application work equally well across all browsers?' If Firefox shows a lower pass rate or slower speeds than Chromium, that's a targeted area for investigation. The radar chart makes cross-browser gaps visually obvious."*

---

## 10. Scoring System

### Benchmark Score (0â€“100)

Each test gets a benchmark score computed from three weighted factors:

| Weight | Factor | How It's Calculated |
|--------|--------|---------------------|
| **40%** | Speed | Duration < 2s = score 100. Duration > 15s = score 0. Linear between. |
| **40%** | Reliability | Passed = 100, Skipped = 50, Failed = 10. Minus 15 points per retry. |
| **20%** | Accessibility | No violations = 100. Penalized: critical Ã— 4 + serious Ã— 3 + moderate Ã— 2 + minor Ã— 1. |

### Overall Suite Score

The suite-level benchmark uses:

| Weight | Factor |
|--------|--------|
| 35% | Pass rate (target: 100%) |
| 30% | Average duration (target: < 2s) |
| 20% | Failure count (target: 0) |
| 15% | Throughput (target: â‰¥ 20 tests/min) |

### Tier Mapping

| Score Range | Tier | Badge Color | Meaning |
|-------------|------|-------------|---------|
| 90 â€“ 100 | ğŸŸ¢ **Elite** | Green | Production-ready, best-in-class quality |
| 75 â€“ 89 | ğŸ”µ **Strong** | Blue | Reliable, minor improvements possible |
| 60 â€“ 74 | ğŸŸ¡ **Stable** | Amber | Acceptable, some optimizations needed |
| 40 â€“ 59 | ğŸŸ  **Watch** | Orange | Needs attention, risks present |
| 0 â€“ 39 | ğŸ”´ **Critical** | Red | Immediate action required |

### Network Score (0â€“100)

| Weight | Factor |
|--------|--------|
| 40% | Request failure rate (target: 0%) |
| 30% | Average response time (target: < 200ms) |
| 30% | HTTP error count (target: 0) |

### Error Score (0â€“100)

| Weight | Factor |
|--------|--------|
| 50% | Console error count (target: 0) |
| 50% | Page error count (target: 0) |

### Observability Score (0â€“100)

| Weight | Factor |
|--------|--------|
| 60% | Network score |
| 40% | Error score |

### Security Score (0â€“100)

| Weight | Factor |
|--------|--------|
| 30% | Observability score |
| 30% | Pass rate |
| 40% | Absence of critical/high findings |

### Security Risk Levels

| Score Range | Risk Level | Meaning |
|-------------|------------|---------|
| 90 â€“ 100 | **Low** | All clear â€” no significant issues |
| 70 â€“ 89 | **Medium** | Minor issues present â€” review recommended |
| 40 â€“ 69 | **High** | Significant concerns â€” action needed |
| 0 â€“ 39 | **Critical** | Urgent remediation required |

### Accessibility Score (0â€“100)

Starts at 100 and is penalized:
```
penalty = (critical Ã— 4) + (serious Ã— 3) + (moderate Ã— 2) + (minor Ã— 1)
score = max(0, 100 - penalty Ã— 5)
```

---

## 11. Interactive Features

| Feature | Where | How to Use |
|---------|-------|------------|
| **Tab navigation** | Top bar | Click any of the 7 tabs to switch views |
| **Test search** | Tests tab | Type in the search box to filter by name, file, or browser |
| **Status filter** | Tests tab | Click All / Passed / Failed / Skipped buttons |
| **Expand test details** | Tests tab | Click any test card header to reveal full details |
| **Screenshot lightbox** | Tests tab | Click any embedded screenshot to zoom full-screen. Click anywhere to close |
| **3D rotation** | Performance tab | Click and drag the 3D scatter plot to rotate in any direction |
| **Chart hover tooltips** | All tabs | Hover over any data point, bar, or slice for detailed tooltip |
| **Responsive layout** | Everywhere | Report adapts from desktop â†’ tablet â†’ mobile widths automatically |
| **Print support** | Everywhere | Use Ctrl+P / Cmd+P for clean printed output |

---

## 12. How to Run

### Prerequisites

| Requirement | Minimum Version |
|-------------|-----------------|
| Node.js | â‰¥ 16 |
| npm | â‰¥ 7 |
| @playwright/test | â‰¥ 1.20 |

No other dependencies are required.

### Step 1 â€” Place the reporter file

Copy `UniversalReporter.ts` into your project:

```
your-project/
â”œâ”€â”€ reporters/
â”‚   â””â”€â”€ UniversalReporter.ts    â† place here
â”œâ”€â”€ tests/
â”œâ”€â”€ playwright.config.ts
â””â”€â”€ package.json
```

### Step 2 â€” Register in playwright.config.ts

```typescript
import { defineConfig } from '@playwright/test';

export default defineConfig({
  reporter: [
    ['list'],                                     // terminal output
    ['./reporters/UniversalReporter.ts'],          // universal report
  ],
  use: {
    screenshot: 'only-on-failure',   // embeds screenshots in report
    trace: 'retain-on-failure',      // shows trace links in report
  },
});
```

**Optional** â€” customize output location:

```typescript
['./reporters/UniversalReporter.ts', {
  outputDir: 'my-reports',        // default: Reports/universal-report
  outputFile: 'report.html',      // default: index.html
}],
```

### Step 3 â€” Run tests

```bash
npx playwright test
```

### Step 4 â€” View the report

```bash
# macOS
open Reports/universal-report/index.html

# Linux
xdg-open Reports/universal-report/index.html

# Windows
start Reports/universal-report/index.html
```

Or serve it locally:

```bash
npx serve Reports/universal-report -l 9876
# Then open http://localhost:9876
```

### Terminal Output

After every run, you'll see a summary in the terminal:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“Š UNIVERSAL REPORT                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  File : /path/to/Reports/universal-report/index.html
â•‘  Size : 3561.2 KB
â•‘  Tests: 10  |  Pass: 10  |  Fail: 0  |  Skip: 0
â•‘  Score: 100/100 [Elite]
â•‘  A11y : 0 violations  (score 100/100)
â•‘  Observability: 441 requests  |  0 console errors  |  0 page errors
â•‘  Network: avg 104.05ms  |  P95 367ms  |  failures 0
â•‘  Security: score 100/100  |  risk Low  |  0 findings
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 13. FAQ

### Q: Does this work without the observability fixture?

**Yes.** The reporter works with any Playwright project out of the box. Without the fixture:
- Dashboard, Tests, Performance, and Browsers tabs work fully
- Observability tab shows zeroes (clean state)
- Security tab shows 100/100 (no issues to report)
- Accessibility tab shows 100/100 (no scans to report)

To get full observability, security, and accessibility data, add the `observability.fixture.ts` to your project.

### Q: How are screenshots embedded?

Screenshots are encoded as base64 and embedded directly in the HTML. No external image files are needed. Configure screenshots in `playwright.config.ts`:

```typescript
use: { screenshot: 'only-on-failure' }
```

### Q: Can I share this report with someone?

Yes â€” it's a single `.html` file. Email it, upload to Slack/Teams, or host it on any web server. The only external dependency is the Plotly.js CDN for charts (loaded on first open).

### Q: What browsers are supported for viewing the report?

Any modern browser: Chrome, Firefox, Safari, Edge. The report uses standard HTML, CSS, and JavaScript.

### Q: Can I integrate this into CI/CD?

Yes. Add `npx playwright test` to your CI pipeline. The report is generated automatically. Common patterns:

- **GitHub Actions:** Upload `Reports/universal-report/` as an artifact
- **Jenkins:** Archive the `Reports/` directory
- **Azure DevOps:** Publish as a pipeline artifact

### Q: How large is the report file?

Typically 500 KB â€“ 5 MB depending on:
- Number of tests (more tests = more data)
- Number of embedded screenshots (each adds ~50â€“200 KB)
- Number of browsers (more browsers = more comparison data)

### Q: What does "Elite" tier mean for our release?

Elite (90â€“100) means:
- Tests run fast (< 2 seconds average)
- All tests pass consistently
- No accessibility violations
- Backend responds quickly
- No console or page errors

This is the standard you should aim for before every production release.

### Q: How is this different from the built-in Playwright HTML report?

| Feature | Playwright HTML | Universal Report |
|---------|----------------|------------------|
| Test results | âœ… | âœ… |
| Steps & errors | âœ… | âœ… |
| Screenshots | âœ… | âœ… (embedded base64) |
| Traces | âœ… | âœ… (link) |
| Benchmark scoring | âŒ | âœ… |
| 3D performance charts | âŒ | âœ… |
| Network observability | âŒ | âœ… |
| Console/page error tracking | âŒ | âœ… |
| Security analysis | âŒ | âœ… |
| Accessibility dashboard | âŒ | âœ… |
| Cross-browser radar | âŒ | âœ… |
| Single file, no server | âŒ (needs `npx playwright show-report`) | âœ… |

---

> **Document version:** 1.0
> **Last updated:** February 2026
> **Reporter file:** `reporters/UniversalReporter.ts` (1,132 lines)
