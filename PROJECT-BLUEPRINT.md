# ðŸ—ï¸ Playwright POM Observability Framework â€” Full Project Blueprint

> **One file to recreate the entire project.** Share this `.md` file with any team member. They follow the steps, paste the code, and they have the full working framework.

---

## Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Step 1 â€” Create the Project Folder](#step-1--create-the-project-folder)
4. [Step 2 â€” Create All Files](#step-2--create-all-files)
5. [Step 3 â€” Install Dependencies](#step-3--install-dependencies)
6. [Step 4 â€” Run Everything](#step-4--run-everything)
7. [Step 5 â€” View Reports](#step-5--view-reports)
8. [Project Structure Reference](#project-structure-reference)
9. [How to Add New Tests](#how-to-add-new-tests)
10. [Troubleshooting](#troubleshooting)

---

## Overview

This framework provides:

- **Playwright + TypeScript** end-to-end testing
- **Page Object Model (POM)** â€” selectors in `pages/`, test logic in `tests/`
- **Automatic observability** â€” every test captures network requests, response times, console/page errors
- **Automatic accessibility scanning** â€” every page is checked for WCAG violations (missing alt text, empty buttons, heading order, form labels, landmarks, color contrast)
- **3 reports from one command** (`npm run reports`):
  1. **Playwright HTML Report** â€” built-in pass/fail with screenshots, videos, traces
  2. **Allure HTML Report** â€” detailed history, categories, attachments
  3. **Performance + Accessibility Benchmark Dashboard** â€” 3D interactive charts, KPI cards, violation list

---

## Prerequisites

- **Node.js** 18+ (LTS recommended)
- **npm** 8+
- **Java** 8+ (required for Allure report generation)
- A terminal (macOS Terminal, iTerm, Windows Terminal, or VS Code integrated terminal)

---

## Step 1 â€” Create the Project Folder

```bash
mkdir playwright-observability-framework
cd playwright-observability-framework
```

Then create the folder structure:

```bash
mkdir -p fixtures observability pages reporters scripts tests
```

---

## Step 2 â€” Create All Files

Create each file below with the exact content shown. The order doesn't matter â€” just make sure every file lands in the correct path.

---

### ðŸ“„ `package.json`

```json
{
  "name": "playwright-observability-framework",
  "version": "1.0.0",
  "private": true,
  "description": "Custom Playwright framework with Page Object Model, metrics instrumentation, and charted observability reports.",
  "scripts": {
    "clean:reports": "node -e \"const fs=require('fs');['Reports','artifacts','allure-results','allure-report'].forEach(p=>fs.rmSync(p,{recursive:true,force:true}));\"",
    "test": "playwright test",
    "test:headed": "playwright test --headed",
    "test:debug": "playwright test --debug",
    "report:playwright": "playwright show-report Reports/playwright-html",
    "report:allure:generate": "node scripts/allure-cli.js generate Reports/allure-results --clean -o Reports/allure-report",
    "report:allure:open": "node scripts/allure-cli.js open Reports/allure-report",
    "report:3d": "node --import tsx scripts/generate-performance-benchmark-report.ts",
    "reports": "sh -c 'npm run clean:reports; npm run test; TEST_EXIT=$?; npm run report:3d; npm run report:allure:generate; exit $TEST_EXIT'",
    "report:open:all": "sh -c 'npx playwright show-report Reports/playwright-html & node scripts/allure-cli.js open Reports/allure-report & open Reports/observability/performance-benchmark-report.html'"
  },
  "devDependencies": {
    "@playwright/test": "^1.58.0",
    "@types/node": "^22.13.10",
    "allure-commandline": "^2.34.1",
    "allure-playwright": "^3.4.5",
    "tsx": "^4.19.2",
    "typescript": "^5.7.3"
  }
}
```

---

### ðŸ“„ `tsconfig.json`

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "CommonJS",
    "moduleResolution": "Node",
    "types": [
      "node",
      "@playwright/test"
    ],
    "strict": true,
    "esModuleInterop": true,
    "resolveJsonModule": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": [
    "**/*.ts"
  ],
  "exclude": [
    "node_modules",
    "artifacts"
  ]
}
```

---

### ðŸ“„ `playwright.config.ts`

```typescript
import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './tests',
  fullyParallel: false,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 1,
  workers: process.env.CI ? 1 : 1,
  timeout: 45_000,
  expect: {
    timeout: 10_000
  },
  reporter: [
    ['list'],
    ['html', { outputFolder: 'Reports/playwright-html', open: 'never' }],
    ['allure-playwright', { resultsDir: 'Reports/allure-results', detail: true, suiteTitle: false }],
    ['./reporters/observability-reporter.ts']
  ],
  outputDir: 'Reports/test-results',
  use: {
    baseURL: process.env.BASE_URL || 'https://playwright.dev',
    trace: 'on',
    screenshot: 'on',
    video: 'on',
    actionTimeout: 10_000,
    navigationTimeout: 45_000
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] }
    },
    {
      name: 'firefox',
      use: { ...devices['Desktop Firefox'] }
    }
  ]
});
```

---

### ðŸ“„ `observability/types.ts`

```typescript
/** A single accessibility violation found on the page. */
export interface AccessibilityViolation {
  id: string;
  impact: 'critical' | 'serious' | 'moderate' | 'minor';
  description: string;
  helpUrl: string;
  nodes: number;
}

/** Summary of an accessibility scan for one test. */
export interface AccessibilityScanResult {
  totalViolations: number;
  critical: number;
  serious: number;
  moderate: number;
  minor: number;
  violations: AccessibilityViolation[];
}

export interface FixtureObservabilityMetrics {
  requestCount: number;
  requestFailureCount: number;
  responseErrorCount: number;
  responseTimesMs: number[];
  avgResponseTimeMs: number;
  p95ResponseTimeMs: number;
  consoleErrors: string[];
  pageErrors: string[];
  testStartedAt: string;
  testEndedAt: string;
  testDurationMs: number;
  accessibility: AccessibilityScanResult;
}

export interface TestObservabilityEntry {
  id: string;
  title: string;
  file: string;
  projectName: string;
  status: string;
  outcome: string;
  durationMs: number;
  retry: number;
  startedAt: string;
  requestCount: number;
  requestFailureCount: number;
  responseErrorCount: number;
  avgResponseTimeMs: number;
  p95ResponseTimeMs: number;
  maxResponseTimeMs: number;
  consoleErrorCount: number;
  pageErrorCount: number;
  accessibility: AccessibilityScanResult;
}

export interface ObservabilitySummary {
  generatedAt: string;
  runId: string;
  overall: {
    totalTests: number;
    passed: number;
    failed: number;
    skipped: number;
    timedOut: number;
    flaky: number;
    avgTestDurationMs: number;
    totalRequests: number;
    requestFailures: number;
    requestFailureRatePct: number;
    avgResponseTimeMs: number;
    p95ResponseTimeMs: number;
  };
  accessibilityOverall: {
    totalViolations: number;
    critical: number;
    serious: number;
    moderate: number;
    minor: number;
    testsWithViolations: number;
    testsScanned: number;
  };
  tests: TestObservabilityEntry[];
}
```

---

### ðŸ“„ `fixtures/observability.fixture.ts`

```typescript
import { promises as fs } from 'node:fs';
import { Request, test as base, expect } from '@playwright/test';
import { AccessibilityScanResult, AccessibilityViolation, FixtureObservabilityMetrics } from '../observability/types';

function average(values: number[]): number {
  if (values.length === 0) {
    return 0;
  }
  return values.reduce((sum, value) => sum + value, 0) / values.length;
}

function percentile(values: number[], p: number): number {
  if (values.length === 0) {
    return 0;
  }
  const sorted = [...values].sort((a, b) => a - b);
  const index = Math.ceil((p / 100) * sorted.length) - 1;
  return sorted[Math.min(Math.max(index, 0), sorted.length - 1)];
}

function round(value: number, digits = 2): number {
  const factor = 10 ** digits;
  return Math.round(value * factor) / factor;
}

/** Run a lightweight accessibility audit using the browser's built-in Accessibility Tree. */
async function runAccessibilityScan(page: import('@playwright/test').Page): Promise<AccessibilityScanResult> {
  const violations: AccessibilityViolation[] = [];

  try {
    // 1. Images without alt text
    const imgsMissingAlt = await page.evaluate(() => {
      const imgs = Array.from(document.querySelectorAll('img'));
      return imgs.filter((img) => !img.getAttribute('alt') && !img.getAttribute('role')).length;
    });
    if (imgsMissingAlt > 0) {
      violations.push({
        id: 'image-alt',
        impact: 'critical',
        description: 'Images must have alternate text',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/image-alt',
        nodes: imgsMissingAlt
      });
    }

    // 2. Buttons / links without accessible names
    const emptyButtons = await page.evaluate(() => {
      const btns = Array.from(document.querySelectorAll('button, [role="button"]'));
      return btns.filter(
        (btn) => !btn.textContent?.trim() && !btn.getAttribute('aria-label') && !btn.getAttribute('aria-labelledby')
      ).length;
    });
    if (emptyButtons > 0) {
      violations.push({
        id: 'button-name',
        impact: 'critical',
        description: 'Buttons must have discernible text',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/button-name',
        nodes: emptyButtons
      });
    }

    const emptyLinks = await page.evaluate(() => {
      const links = Array.from(document.querySelectorAll('a[href]'));
      return links.filter(
        (a) => !a.textContent?.trim() && !a.getAttribute('aria-label') && !a.getAttribute('aria-labelledby') && !a.querySelector('img[alt]')
      ).length;
    });
    if (emptyLinks > 0) {
      violations.push({
        id: 'link-name',
        impact: 'serious',
        description: 'Links must have discernible text',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/link-name',
        nodes: emptyLinks
      });
    }

    // 3. Missing document language
    const missingLang = await page.evaluate(() => !document.documentElement.getAttribute('lang'));
    if (missingLang) {
      violations.push({
        id: 'html-has-lang',
        impact: 'serious',
        description: '<html> element must have a lang attribute',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/html-has-lang',
        nodes: 1
      });
    }

    // 4. Form inputs without labels
    const unlabeledInputs = await page.evaluate(() => {
      const inputs = Array.from(document.querySelectorAll('input, select, textarea'));
      return inputs.filter((el) => {
        const id = el.getAttribute('id');
        const hasLabel = id ? document.querySelector(`label[for="${id}"]`) : false;
        return !hasLabel && !el.getAttribute('aria-label') && !el.getAttribute('aria-labelledby');
      }).length;
    });
    if (unlabeledInputs > 0) {
      violations.push({
        id: 'label',
        impact: 'critical',
        description: 'Form elements must have labels',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/label',
        nodes: unlabeledInputs
      });
    }

    // 5. Heading order / skipped heading levels
    const skippedHeadings = await page.evaluate(() => {
      const headings = Array.from(document.querySelectorAll('h1,h2,h3,h4,h5,h6'));
      let skips = 0;
      let prev = 0;
      for (const h of headings) {
        const level = parseInt(h.tagName[1], 10);
        if (prev > 0 && level > prev + 1) skips++;
        prev = level;
      }
      return skips;
    });
    if (skippedHeadings > 0) {
      violations.push({
        id: 'heading-order',
        impact: 'moderate',
        description: 'Heading levels should increase by one',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/heading-order',
        nodes: skippedHeadings
      });
    }

    // 6. Color contrast â€” flag elements with very small text and no explicit contrast
    const lowContrastCandidates = await page.evaluate(() => {
      const elements = Array.from(document.querySelectorAll('p, span, a, li, td, th, label'));
      let count = 0;
      for (const el of elements) {
        const style = window.getComputedStyle(el);
        const fontSize = parseFloat(style.fontSize);
        const color = style.color;
        const bg = style.backgroundColor;
        if (fontSize < 12 && color === bg) count++;
      }
      return count;
    });
    if (lowContrastCandidates > 0) {
      violations.push({
        id: 'color-contrast',
        impact: 'serious',
        description: 'Elements must have sufficient color contrast',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/color-contrast',
        nodes: lowContrastCandidates
      });
    }

    // 7. Missing landmark regions
    const hasMain = await page.evaluate(() => !!document.querySelector('main, [role="main"]'));
    if (!hasMain) {
      violations.push({
        id: 'landmark-main-is-top-level',
        impact: 'moderate',
        description: 'Page should contain a main landmark',
        helpUrl: 'https://dequeuniversity.com/rules/axe/4.10/landmark-main-is-top-level',
        nodes: 1
      });
    }
  } catch {
    // If scan fails (e.g., page closed early), return empty results
  }

  const critical = violations.filter((v) => v.impact === 'critical').length;
  const serious = violations.filter((v) => v.impact === 'serious').length;
  const moderate = violations.filter((v) => v.impact === 'moderate').length;
  const minor = violations.filter((v) => v.impact === 'minor').length;

  return {
    totalViolations: violations.length,
    critical,
    serious,
    moderate,
    minor,
    violations
  };
}

export const test = base.extend<{ observabilityAuto: void }>({
  observabilityAuto: [
    async ({ page }, use, testInfo) => {
      const requestStartedAt = new Map<Request, number>();
      const responseTimes: number[] = [];
      const consoleErrors: string[] = [];
      const pageErrors: string[] = [];

      let requestCount = 0;
      let requestFailureCount = 0;
      let responseErrorCount = 0;

      const testStartedAtMs = Date.now();

      page.on('request', (request) => {
        requestCount += 1;
        requestStartedAt.set(request, Date.now());
      });

      page.on('requestfinished', (request) => {
        const startedAt = requestStartedAt.get(request);
        if (startedAt) {
          responseTimes.push(Date.now() - startedAt);
          requestStartedAt.delete(request);
        }
      });

      page.on('requestfailed', (request) => {
        requestFailureCount += 1;
        requestStartedAt.delete(request);
      });

      page.on('response', (response) => {
        if (response.status() >= 400) {
          responseErrorCount += 1;
        }
      });

      page.on('console', (message) => {
        if (message.type() === 'error') {
          consoleErrors.push(message.text());
        }
      });

      page.on('pageerror', (error) => {
        pageErrors.push(error.message);
      });

      await use();

      // Run accessibility scan after test completes
      let accessibility: AccessibilityScanResult = {
        totalViolations: 0, critical: 0, serious: 0, moderate: 0, minor: 0, violations: []
      };
      try {
        accessibility = await runAccessibilityScan(page);
      } catch {
        // Page may have been closed
      }

      const testEndedAtMs = Date.now();

      const metrics: FixtureObservabilityMetrics = {
        requestCount,
        requestFailureCount,
        responseErrorCount,
        responseTimesMs: responseTimes,
        avgResponseTimeMs: round(average(responseTimes)),
        p95ResponseTimeMs: round(percentile(responseTimes, 95)),
        consoleErrors,
        pageErrors,
        testStartedAt: new Date(testStartedAtMs).toISOString(),
        testEndedAt: new Date(testEndedAtMs).toISOString(),
        testDurationMs: testEndedAtMs - testStartedAtMs,
        accessibility
      };

      const attachmentPath = testInfo.outputPath('observability-metrics.json');
      await fs.writeFile(attachmentPath, JSON.stringify(metrics, null, 2), 'utf-8');
      await testInfo.attach('observability-metrics', {
        path: attachmentPath,
        contentType: 'application/json'
      });
    },
    { auto: true }
  ]
});

export { expect };
```

---

### ðŸ“„ `fixtures/test.fixture.ts`

```typescript
import { test as base, expect } from './observability.fixture';
import { GettingStartedVscodePage } from '../pages/GettingStartedVscodePage';

type AppFixtures = {
  docsPage: GettingStartedVscodePage;
};

export const test = base.extend<AppFixtures>({
  docsPage: async ({ page }, use) => {
    await use(new GettingStartedVscodePage(page));
  }
});

export { expect };
```

---

### ðŸ“„ `pages/GettingStartedVscodePage.ts`

```typescript
import { expect, Locator, Page } from '@playwright/test';

export class GettingStartedVscodePage {
  private readonly page: Page;
  readonly sidebarMenu: Locator;
  readonly tocLinks: Locator;

  constructor(page: Page) {
    this.page = page;
    this.sidebarMenu = page.locator('.theme-doc-sidebar-menu');
    this.tocLinks = page.locator('.theme-doc-toc-desktop a.table-of-contents__link[href^="#"]');
  }

  async open(): Promise<void> {
    await this.page.goto('/docs/getting-started-vscode', { waitUntil: 'domcontentloaded' });
  }

  async assertTitleAndMainHeading(): Promise<void> {
    await expect(this.page).toHaveTitle(/Getting started - VS Code/i);
    await expect(this.page.getByRole('heading', { level: 1, name: /Getting started - VS Code/i })).toBeVisible();
  }

  async assertTopSections(): Promise<void> {
    await expect(this.page.getByRole('heading', { level: 2, name: /Introduction/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 2, name: /Prerequisites/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 2, name: /Getting Started/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 2, name: /Core Features/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 2, name: /Advanced Features/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 2, name: /Quick Reference/i })).toBeVisible();
  }

  async assertCoreSubSections(): Promise<void> {
    await expect(this.page.getByRole('heading', { level: 3, name: /Installation & Setup/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 3, name: /Opening the Testing Sidebar/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 3, name: /Running Your Tests/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 3, name: /Debugging Your Tests/i })).toBeVisible();
    await expect(this.page.getByRole('heading', { level: 3, name: /Generating Tests with CodeGen/i })).toBeVisible();
  }

  async assertSidebarLinks(): Promise<void> {
    await expect(this.sidebarMenu).toBeVisible();
    await expect(this.sidebarMenu.getByText(/^Getting Started$/i)).toBeVisible();
    await expect(this.sidebarMenu.getByRole('link', { name: /^Getting started - VS Code$/i })).toBeVisible();
    await expect(this.sidebarMenu.getByRole('link', { name: /^Running and debugging tests$/i })).toBeVisible();
  }

  async assertTocAnchorsResolve(maxAnchors = 8): Promise<void> {
    const count = await this.tocLinks.count();
    expect(count).toBeGreaterThan(6);

    const hrefs = await this.tocLinks.evaluateAll((links) => {
      const values = links
        .map((link) => link.getAttribute('href') || '')
        .filter((href) => href.startsWith('#'));
      return Array.from(new Set(values));
    });

    expect(hrefs.length).toBeGreaterThan(4);

    for (const href of hrefs.slice(0, maxAnchors)) {
      await expect(this.page.locator(href).first(), `Expected section for ${href}`).toBeAttached();
    }
  }
}
```

---

### ðŸ“„ `tests/getting-started-vscode.spec.ts`

```typescript
import { test } from '../fixtures/test.fixture';

test.describe('Playwright docs - getting started with VS Code', () => {
  test.beforeEach(async ({ docsPage }) => {
    await docsPage.open();
  });

  test('loads the page with expected title and heading', async ({ docsPage }) => {
    await docsPage.assertTitleAndMainHeading();
  });

  test('shows the top-level documentation sections', async ({ docsPage }) => {
    await docsPage.assertTopSections();
  });

  test('contains setup and execution subsections', async ({ docsPage }) => {
    await docsPage.assertCoreSubSections();
  });

  test('left sidebar includes active docs entry and key navigation links', async ({ docsPage }) => {
    await docsPage.assertSidebarLinks();
  });

  test('table of contents links point to existing sections', async ({ docsPage }) => {
    await docsPage.assertTocAnchorsResolve();
  });
});
```

---

### ðŸ“„ `reporters/observability-reporter.ts`

```typescript
import path from 'node:path';
import { promises as fs } from 'node:fs';
import type { Reporter, FullResult, TestCase, TestResult } from '@playwright/test/reporter';
import type {
  AccessibilityScanResult,
  FixtureObservabilityMetrics,
  ObservabilitySummary,
  TestObservabilityEntry
} from '../observability/types';

const OUTPUT_DIR = path.resolve(process.cwd(), 'Reports/observability');
const OUTPUT_FILE = path.resolve(OUTPUT_DIR, 'observability-metrics.json');

const EMPTY_A11Y: AccessibilityScanResult = {
  totalViolations: 0,
  critical: 0,
  serious: 0,
  moderate: 0,
  minor: 0,
  violations: []
};

function average(values: number[]): number {
  if (values.length === 0) {
    return 0;
  }
  return values.reduce((sum, value) => sum + value, 0) / values.length;
}

function percentile(values: number[], p: number): number {
  if (values.length === 0) {
    return 0;
  }
  const sorted = [...values].sort((a, b) => a - b);
  const index = Math.ceil((p / 100) * sorted.length) - 1;
  return sorted[Math.min(Math.max(index, 0), sorted.length - 1)];
}

function round(value: number, digits = 2): number {
  const factor = 10 ** digits;
  return Math.round(value * factor) / factor;
}

function resolveProjectName(test: TestCase): string {
  const fromSuite = (test.parent as { project?: () => { name: string } | undefined } | undefined)
    ?.project?.()
    ?.name;
  if (fromSuite) {
    return fromSuite;
  }

  const firstPathSegment = test.titlePath()[0];
  if (firstPathSegment && ['chromium', 'firefox', 'webkit'].includes(firstPathSegment.toLowerCase())) {
    return firstPathSegment.toLowerCase();
  }

  const fullTitle = test.titlePath().find((segment) => segment.startsWith('[') && segment.endsWith(']'));
  if (!fullTitle) {
    return 'default';
  }
  return fullTitle.slice(1, -1);
}

async function readMetricsAttachment(result: TestResult): Promise<FixtureObservabilityMetrics | undefined> {
  const attachment = result.attachments.find((item) => item.name === 'observability-metrics' && item.path);
  if (!attachment?.path) {
    return undefined;
  }

  try {
    const raw = await fs.readFile(attachment.path, 'utf-8');
    return JSON.parse(raw) as FixtureObservabilityMetrics;
  } catch {
    return undefined;
  }
}

class ObservabilityReporter implements Reporter {
  private readonly runId = new Date().toISOString().replace(/[:.]/g, '-');
  private readonly testsById = new Map<string, TestObservabilityEntry>();

  async onTestEnd(test: TestCase, result: TestResult): Promise<void> {
    const metrics = await readMetricsAttachment(result);
    const responseTimes = metrics?.responseTimesMs ?? [];
    const maxResponseTimeMs = responseTimes.length > 0 ? Math.max(...responseTimes) : 0;
    const titlePath = test.titlePath().filter(Boolean);

    const entry: TestObservabilityEntry = {
      id: test.id,
      title: titlePath.join(' > ') || test.title,
      file: test.location.file,
      projectName: resolveProjectName(test),
      status: result.status,
      outcome: test.outcome(),
      durationMs: result.duration,
      retry: result.retry,
      startedAt: result.startTime.toISOString(),
      requestCount: metrics?.requestCount ?? 0,
      requestFailureCount: metrics?.requestFailureCount ?? 0,
      responseErrorCount: metrics?.responseErrorCount ?? 0,
      avgResponseTimeMs: metrics?.avgResponseTimeMs ?? 0,
      p95ResponseTimeMs: metrics?.p95ResponseTimeMs ?? 0,
      maxResponseTimeMs,
      consoleErrorCount: metrics?.consoleErrors.length ?? 0,
      pageErrorCount: metrics?.pageErrors.length ?? 0,
      accessibility: metrics?.accessibility ?? EMPTY_A11Y
    };

    this.testsById.set(test.id, entry);
  }

  async onEnd(result: FullResult): Promise<void> {
    const tests = Array.from(this.testsById.values()).sort((left, right) => right.durationMs - left.durationMs);
    const totalTests = tests.length;
    const passed = tests.filter((item) => item.status === 'passed').length;
    const failed = tests.filter((item) => item.status === 'failed').length;
    const skipped = tests.filter((item) => item.status === 'skipped').length;
    const timedOut = tests.filter((item) => item.status === 'timedOut').length;
    const flaky = tests.filter((item) => item.outcome === 'flaky').length;
    const totalRequests = tests.reduce((sum, item) => sum + item.requestCount, 0);
    const requestFailures = tests.reduce((sum, item) => sum + item.requestFailureCount, 0);

    // Aggregate accessibility data
    const a11yOverall = {
      totalViolations: tests.reduce((sum, t) => sum + t.accessibility.totalViolations, 0),
      critical: tests.reduce((sum, t) => sum + t.accessibility.critical, 0),
      serious: tests.reduce((sum, t) => sum + t.accessibility.serious, 0),
      moderate: tests.reduce((sum, t) => sum + t.accessibility.moderate, 0),
      minor: tests.reduce((sum, t) => sum + t.accessibility.minor, 0),
      testsWithViolations: tests.filter((t) => t.accessibility.totalViolations > 0).length,
      testsScanned: tests.length
    };

    const summary: ObservabilitySummary = {
      generatedAt: new Date().toISOString(),
      runId: this.runId,
      overall: {
        totalTests,
        passed,
        failed,
        skipped,
        timedOut,
        flaky,
        avgTestDurationMs: round(average(tests.map((item) => item.durationMs))),
        totalRequests,
        requestFailures,
        requestFailureRatePct: totalRequests === 0 ? 0 : round((requestFailures / totalRequests) * 100),
        avgResponseTimeMs: round(average(tests.map((item) => item.avgResponseTimeMs).filter((value) => value > 0))),
        p95ResponseTimeMs: round(percentile(tests.map((item) => item.p95ResponseTimeMs).filter((value) => value > 0), 95))
      },
      accessibilityOverall: a11yOverall,
      tests
    };

    await fs.mkdir(OUTPUT_DIR, { recursive: true });
    await fs.writeFile(OUTPUT_FILE, JSON.stringify(summary, null, 2), 'utf-8');

    console.log(`\n[observability] Metrics written to ${OUTPUT_FILE}`);
    console.log(`[observability] Accessibility: ${a11yOverall.totalViolations} violations (${a11yOverall.critical} critical, ${a11yOverall.serious} serious)`);
    console.log(`[observability] Playwright run status: ${result.status}\n`);
  }
}

export default ObservabilityReporter;
```

---

### ðŸ“„ `scripts/allure-cli.js`

> **Why this file exists:** The `allure-commandline` npm package has a bug where it doesn't quote the binary path when spawning a shell. If your project folder path contains a space (e.g., `My Project`), the allure command breaks. This wrapper fixes that.

```javascript
#!/usr/bin/env node
/**
 * Wrapper around allure-commandline that handles paths with spaces correctly.
 * The upstream allure-commandline package has a bug where it uses shell: true
 * without quoting the binary path, breaking on directories with spaces.
 */
const path = require('path');
const { spawn } = require('child_process');

const allureBin = path.join(
  __dirname,
  '..',
  'node_modules',
  'allure-commandline',
  'dist',
  'bin',
  'allure'
);

const child = spawn(`"${allureBin}"`, process.argv.slice(2), {
  env: process.env,
  stdio: 'inherit',
  shell: true,
});

child.on('close', (code) => {
  process.exit(code || 0);
});
```

---

### ðŸ“„ `scripts/generate-performance-benchmark-report.ts`

> âš ï¸ **This is the largest file (~960 lines).** It reads the raw observability JSON and generates a full interactive HTML dashboard with 3D charts, KPI cards, accessibility data, and per-test tables.

```typescript
import path from 'node:path';
import { promises as fs } from 'node:fs';
import {
  AccessibilityScanResult,
  ObservabilitySummary,
  TestObservabilityEntry
} from '../observability/types';

const DEFAULT_INPUT = path.resolve(process.cwd(), 'Reports/observability/observability-metrics.json');
const OUTPUT_DIR = path.resolve(process.cwd(), 'Reports/observability');
const OUTPUT_FILE = path.resolve(OUTPUT_DIR, 'performance-benchmark-report.html');

// ---------------------------------------------------------------------------
//  Types
// ---------------------------------------------------------------------------

type BenchmarkTier = 'Elite' | 'Strong' | 'Stable' | 'Watch' | 'Critical';

interface BenchmarkTestEntry extends TestObservabilityEntry {
  browser: string;
  errorSignals: number;
  throughputPerMin: number;
  durationScore: number;
  reliabilityScore: number;
  qualityScore: number;
  throughputScore: number;
  accessibilityScore: number;
  benchmarkScore: number;
  benchmarkTier: BenchmarkTier;
  startedAtMs: number;
  endedAtMs: number;
}

interface BrowserBenchmarkRow {
  browser: string;
  totalTests: number;
  passed: number;
  failed: number;
  totalRequests: number;
  requestFailures: number;
  requestFailureRatePct: number;
  avgResponseTimeMs: number;
  p95ResponseTimeMs: number;
  passRatePct: number;
  retryRatePct: number;
  errorSignalsPerTest: number;
  avgDurationMs: number;
  medianDurationMs: number;
  p95DurationMs: number;
  p99DurationMs: number;
  stdDevDurationMs: number;
  cvPct: number;
  throughputTestsPerMin: number;
  durationScore: number;
  reliabilityScore: number;
  qualityScore: number;
  throughputScore: number;
  accessibilityScore: number;
  benchmarkScore: number;
  benchmarkTier: BenchmarkTier;
  a11yViolations: number;
  a11yCritical: number;
  a11ySerious: number;
}

interface BenchmarkPayload {
  generatedAt: string;
  runId: string;
  thresholds: {
    avgDurationTargetMs: number;
    p95DurationTargetMs: number;
    throughputTargetPerMin: number;
    retryRateTargetPct: number;
  };
  overall: {
    totalTests: number;
    totalRequests: number;
    requestFailures: number;
    requestFailureRatePct: number;
    avgResponseTimeMs: number;
    p95ResponseTimeMs: number;
    consoleErrorCount: number;
    pageErrorCount: number;
    passRatePct: number;
    medianDurationMs: number;
    p90DurationMs: number;
    p99DurationMs: number;
    stdDevDurationMs: number;
    cvPct: number;
    throughputTestsPerMin: number;
    retryRatePct: number;
    errorSignalsPerTest: number;
    benchmarkScore: number;
    benchmarkTier: BenchmarkTier;
  };
  accessibility: {
    totalViolations: number;
    critical: number;
    serious: number;
    moderate: number;
    minor: number;
    testsWithViolations: number;
    testsScanned: number;
    accessibilityScore: number;
    topViolations: { id: string; impact: string; description: string; count: number }[];
  };
  browserRows: BrowserBenchmarkRow[];
  tests: BenchmarkTestEntry[];
}

// ---------------------------------------------------------------------------
//  Math helpers
// ---------------------------------------------------------------------------

function round(value: number, digits = 2): number {
  const factor = 10 ** digits;
  return Math.round(value * factor) / factor;
}

function clamp(value: number, min: number, max: number): number {
  return Math.min(max, Math.max(min, value));
}

function average(values: number[]): number {
  if (values.length === 0) return 0;
  return values.reduce((s, v) => s + v, 0) / values.length;
}

function percentile(values: number[], p: number): number {
  if (values.length === 0) return 0;
  const sorted = [...values].sort((a, b) => a - b);
  const idx = Math.ceil((p / 100) * sorted.length) - 1;
  return sorted[Math.min(Math.max(idx, 0), sorted.length - 1)];
}

function stdDev(values: number[]): number {
  if (values.length === 0) return 0;
  const mean = average(values);
  return Math.sqrt(average(values.map((v) => (v - mean) ** 2)));
}

// ---------------------------------------------------------------------------
//  Scoring helpers
// ---------------------------------------------------------------------------

function scoreLowerBetter(value: number, target: number, max: number): number {
  if (value <= target) return 100;
  if (value >= max) return 0;
  return round(clamp((1 - (value - target) / (max - target)) * 100, 0, 100));
}

function scoreHigherBetter(value: number, min: number, target: number): number {
  if (value >= target) return 100;
  if (value <= min) return 0;
  return round(clamp(((value - min) / (target - min)) * 100, 0, 100));
}

function tierFromScore(score: number): BenchmarkTier {
  if (score >= 90) return 'Elite';
  if (score >= 75) return 'Strong';
  if (score >= 60) return 'Stable';
  if (score >= 40) return 'Watch';
  return 'Critical';
}

function a11yScore(a: AccessibilityScanResult): number {
  const penalty = a.critical * 4 + a.serious * 3 + a.moderate * 2 + a.minor * 1;
  return round(clamp(100 - penalty * 8, 0, 100));
}

// ---------------------------------------------------------------------------
//  Data extraction
// ---------------------------------------------------------------------------

function extractBrowserName(test: TestObservabilityEntry): string {
  const head = test.title.split(' > ')[0].trim().toLowerCase();
  if (['chromium', 'firefox', 'webkit'].includes(head)) return head;
  if (test.projectName && test.projectName !== 'default') return test.projectName.toLowerCase();
  return 'unknown';
}

function wallTimeMsFromTests(tests: Pick<BenchmarkTestEntry, 'startedAtMs' | 'endedAtMs' | 'durationMs'>[]): number {
  if (tests.length === 0) return 0;
  const starts = tests.map((t) => t.startedAtMs).filter(Number.isFinite);
  const ends = tests.map((t) => t.endedAtMs).filter(Number.isFinite);
  if (starts.length > 0 && ends.length > 0) {
    const span = Math.max(...ends) - Math.min(...starts);
    if (span > 0) return span;
  }
  return tests.reduce((s, t) => s + t.durationMs, 0);
}

// ---------------------------------------------------------------------------
//  Build enriched entries
// ---------------------------------------------------------------------------

function buildBenchmarkTestEntry(test: TestObservabilityEntry): BenchmarkTestEntry {
  const browser = extractBrowserName(test);
  const errorSignals =
    test.requestFailureCount + test.responseErrorCount + test.consoleErrorCount + test.pageErrorCount;
  const throughputPerMin = round(60_000 / Math.max(1, test.durationMs));

  const durationScore = scoreLowerBetter(test.durationMs, 1_500, 9_000);
  const reliabilityBase = test.status === 'passed' ? 100 : test.status === 'skipped' ? 70 : 15;
  const reliabilityScore = clamp(reliabilityBase - test.retry * 15, 0, 100);
  const qualityScore = scoreLowerBetter(errorSignals, 0, 6);
  const throughputScore = scoreHigherBetter(throughputPerMin, 4, 30);
  const accessibilityScore = a11yScore(test.accessibility);

  const benchmarkScore = round(
    durationScore * 0.35 +
    reliabilityScore * 0.25 +
    qualityScore * 0.15 +
    throughputScore * 0.1 +
    accessibilityScore * 0.15
  );

  const startedAtMs = Date.parse(test.startedAt);
  const endedAtMs = Number.isFinite(startedAtMs) ? startedAtMs + test.durationMs : test.durationMs;

  return {
    ...test,
    browser,
    errorSignals,
    throughputPerMin,
    durationScore: round(durationScore),
    reliabilityScore: round(reliabilityScore),
    qualityScore: round(qualityScore),
    throughputScore: round(throughputScore),
    accessibilityScore: round(accessibilityScore),
    benchmarkScore,
    benchmarkTier: tierFromScore(benchmarkScore),
    startedAtMs,
    endedAtMs
  };
}

function buildBrowserRows(tests: BenchmarkTestEntry[]): BrowserBenchmarkRow[] {
  const groups = new Map<string, BenchmarkTestEntry[]>();
  for (const t of tests) {
    const list = groups.get(t.browser) ?? [];
    list.push(t);
    groups.set(t.browser, list);
  }

  const rows: BrowserBenchmarkRow[] = [];
  for (const [browser, list] of groups.entries()) {
    const durations = list.map((i) => i.durationMs);
    const responseTimes = list.map((i) => i.avgResponseTimeMs).filter((v) => v > 0);
    const passRatePct = list.length === 0 ? 0 : (list.filter((i) => i.status === 'passed').length / list.length) * 100;
    const retryRatePct = list.length === 0 ? 0 : (list.filter((i) => i.retry > 0).length / list.length) * 100;
    const totalRequests = list.reduce((s, i) => s + i.requestCount, 0);
    const requestFailures = list.reduce((s, i) => s + i.requestFailureCount, 0);
    const errorSignalsPerTest = list.length === 0 ? 0 : average(list.map((i) => i.errorSignals));
    const avgDurationMs = average(durations);
    const p95DurationMs = percentile(durations, 95);
    const throughputTestsPerMin = list.length === 0 ? 0 : list.length / (wallTimeMsFromTests(list) / 60_000);

    const durationScore = round(
      scoreLowerBetter(avgDurationMs, 1_500, 7_000) * 0.6 +
      scoreLowerBetter(p95DurationMs, 3_000, 9_500) * 0.4
    );
    const reliabilityScore = round(passRatePct * 0.7 + scoreLowerBetter(retryRatePct, 0, 40) * 0.3);
    const qualityScore = round(scoreLowerBetter(errorSignalsPerTest, 0, 4));
    const throughputScore = round(scoreHigherBetter(throughputTestsPerMin, 5, 22));
    const accessibilityScoreVal = round(average(list.map((i) => i.accessibilityScore)));
    const benchmarkScore = round(
      durationScore * 0.3 +
      reliabilityScore * 0.25 +
      qualityScore * 0.15 +
      throughputScore * 0.1 +
      accessibilityScoreVal * 0.2
    );

    const a11yViolations = list.reduce((s, i) => s + i.accessibility.totalViolations, 0);
    const a11yCritical = list.reduce((s, i) => s + i.accessibility.critical, 0);
    const a11ySerious = list.reduce((s, i) => s + i.accessibility.serious, 0);

    rows.push({
      browser,
      totalTests: list.length,
      passed: list.filter((i) => i.status === 'passed').length,
      failed: list.filter((i) => i.status === 'failed' || i.status === 'timedOut').length,
      totalRequests,
      requestFailures,
      requestFailureRatePct: totalRequests > 0 ? round((requestFailures / totalRequests) * 100) : 0,
      avgResponseTimeMs: round(average(responseTimes)),
      p95ResponseTimeMs: round(percentile(responseTimes, 95)),
      passRatePct: round(passRatePct),
      retryRatePct: round(retryRatePct),
      errorSignalsPerTest: round(errorSignalsPerTest),
      avgDurationMs: round(avgDurationMs),
      medianDurationMs: round(percentile(durations, 50)),
      p95DurationMs: round(p95DurationMs),
      p99DurationMs: round(percentile(durations, 99)),
      stdDevDurationMs: round(stdDev(durations)),
      cvPct: avgDurationMs > 0 ? round((stdDev(durations) / avgDurationMs) * 100) : 0,
      throughputTestsPerMin: round(throughputTestsPerMin),
      durationScore,
      reliabilityScore,
      qualityScore,
      throughputScore,
      accessibilityScore: accessibilityScoreVal,
      benchmarkScore,
      benchmarkTier: tierFromScore(benchmarkScore),
      a11yViolations,
      a11yCritical,
      a11ySerious
    });
  }

  return rows.sort((a, b) => b.benchmarkScore - a.benchmarkScore);
}

// ---------------------------------------------------------------------------
//  Build payload
// ---------------------------------------------------------------------------

function buildPayload(summary: ObservabilitySummary): BenchmarkPayload {
  const tests = summary.tests.map(buildBenchmarkTestEntry);
  const durations = tests.map((i) => i.durationMs);
  const responseTimes = tests.map((i) => i.avgResponseTimeMs).filter((v) => v > 0);
  const browserRows = buildBrowserRows(tests);
  const passRatePct = tests.length === 0 ? 0 : (tests.filter((i) => i.status === 'passed').length / tests.length) * 100;
  const retryRatePct = tests.length === 0 ? 0 : (tests.filter((i) => i.retry > 0).length / tests.length) * 100;
  const errorSignalsPerTest = tests.length === 0 ? 0 : average(tests.map((i) => i.errorSignals));
  const totalRequests = tests.reduce((s, i) => s + i.requestCount, 0);
  const requestFailures = tests.reduce((s, i) => s + i.requestFailureCount, 0);
  const consoleErrorCount = tests.reduce((s, i) => s + i.consoleErrorCount, 0);
  const pageErrorCount = tests.reduce((s, i) => s + i.pageErrorCount, 0);
  const throughputTestsPerMin = tests.length === 0 ? 0 : tests.length / (wallTimeMsFromTests(tests) / 60_000);
  const avgDurationMs = average(durations);
  const p95DurationMs = percentile(durations, 95);
  const durationScore = round(
    scoreLowerBetter(avgDurationMs, 1_500, 7_000) * 0.6 + scoreLowerBetter(p95DurationMs, 3_000, 9_500) * 0.4
  );
  const reliabilityScore = round(passRatePct * 0.7 + scoreLowerBetter(retryRatePct, 0, 40) * 0.3);
  const qualityScore = round(scoreLowerBetter(errorSignalsPerTest, 0, 4));
  const throughputScore = round(scoreHigherBetter(throughputTestsPerMin, 5, 22));
  const benchmarkScore = round(
    durationScore * 0.4 + reliabilityScore * 0.3 + qualityScore * 0.2 + throughputScore * 0.1
  );

  // Accessibility aggregation
  const a11y = summary.accessibilityOverall;
  const overallA11yScore = round(average(tests.map((t) => t.accessibilityScore)));

  // Aggregate top violations across all tests
  const violationMap = new Map<string, { id: string; impact: string; description: string; count: number }>();
  for (const t of tests) {
    for (const v of t.accessibility.violations) {
      const existing = violationMap.get(v.id);
      if (existing) {
        existing.count += v.nodes;
      } else {
        violationMap.set(v.id, { id: v.id, impact: v.impact, description: v.description, count: v.nodes });
      }
    }
  }
  const topViolations = Array.from(violationMap.values()).sort((a, b) => b.count - a.count).slice(0, 10);

  return {
    generatedAt: new Date().toISOString(),
    runId: summary.runId,
    thresholds: {
      avgDurationTargetMs: 1500,
      p95DurationTargetMs: 3000,
      throughputTargetPerMin: 22,
      retryRateTargetPct: 0
    },
    overall: {
      totalTests: summary.overall.totalTests,
      totalRequests,
      requestFailures,
      requestFailureRatePct: totalRequests > 0 ? round((requestFailures / totalRequests) * 100) : 0,
      avgResponseTimeMs: round(average(responseTimes)),
      p95ResponseTimeMs: round(percentile(responseTimes, 95)),
      consoleErrorCount,
      pageErrorCount,
      passRatePct: round(passRatePct),
      medianDurationMs: round(percentile(durations, 50)),
      p90DurationMs: round(percentile(durations, 90)),
      p99DurationMs: round(percentile(durations, 99)),
      stdDevDurationMs: round(stdDev(durations)),
      cvPct: avgDurationMs > 0 ? round((stdDev(durations) / avgDurationMs) * 100) : 0,
      throughputTestsPerMin: round(throughputTestsPerMin),
      retryRatePct: round(retryRatePct),
      errorSignalsPerTest: round(errorSignalsPerTest),
      benchmarkScore,
      benchmarkTier: tierFromScore(benchmarkScore)
    },
    accessibility: {
      totalViolations: a11y.totalViolations,
      critical: a11y.critical,
      serious: a11y.serious,
      moderate: a11y.moderate,
      minor: a11y.minor,
      testsWithViolations: a11y.testsWithViolations,
      testsScanned: a11y.testsScanned,
      accessibilityScore: overallA11yScore,
      topViolations
    },
    browserRows,
    tests
  };
}

// ---------------------------------------------------------------------------
//  HTML Builder
// ---------------------------------------------------------------------------

function buildHtml(payload: BenchmarkPayload): string {
  const safePayload = JSON.stringify(payload).replace(/</g, '\\u003c');

  return `<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance &amp; Accessibility Benchmark Report</title>
  <script src="https://cdn.plot.ly/plotly-2.35.2.min.js"><\/script>
  <style>
    :root {
      --bg: #f6f8fc;
      --surface: #ffffff;
      --ink: #1a1f36;
      --muted: #6b7280;
      --accent: #6366f1;
      --accent-soft: #eef2ff;
      --green: #059669;
      --green-soft: #ecfdf5;
      --amber: #d97706;
      --amber-soft: #fffbeb;
      --red: #dc2626;
      --red-soft: #fef2f2;
      --blue: #2563eb;
      --blue-soft: #eff6ff;
      --purple: #7c3aed;
      --purple-soft: #f5f3ff;
      --border: #e5e7eb;
      --radius: 12px;
      --radius-sm: 8px;
      --shadow: 0 1px 3px rgba(0,0,0,0.08), 0 1px 2px rgba(0,0,0,0.06);
      --shadow-lg: 0 4px 12px rgba(0,0,0,0.1);
    }
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Segoe UI", Roboto, sans-serif;
      color: var(--ink);
      background: var(--bg);
      line-height: 1.6;
      -webkit-font-smoothing: antialiased;
    }
    .container { max-width: 1400px; margin: 0 auto; padding: 24px; }
    .header {
      background: linear-gradient(135deg, #1e1b4b 0%, #312e81 50%, #4338ca 100%);
      color: #fff;
      border-radius: var(--radius);
      padding: 32px;
      margin-bottom: 24px;
      box-shadow: var(--shadow-lg);
    }
    .header h1 { font-size: 1.75rem; font-weight: 700; letter-spacing: -0.02em; margin-bottom: 8px; }
    .header .meta { display: flex; flex-wrap: wrap; gap: 20px; font-size: 0.875rem; color: rgba(255,255,255,0.8); }
    .header .meta strong { color: #fff; }
    .section-title {
      font-size: 1.125rem; font-weight: 700; color: var(--ink);
      margin: 28px 0 14px; padding-bottom: 8px;
      border-bottom: 2px solid var(--accent);
      display: flex; align-items: center; gap: 8px;
    }
    .section-title .icon { font-size: 1.2rem; }
    .kpi-grid {
      display: grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
      gap: 14px; margin-bottom: 24px;
    }
    .kpi {
      background: var(--surface); border: 1px solid var(--border);
      border-radius: var(--radius); padding: 18px; box-shadow: var(--shadow);
      transition: transform 0.15s ease, box-shadow 0.15s ease;
    }
    .kpi:hover { transform: translateY(-2px); box-shadow: var(--shadow-lg); }
    .kpi-label {
      font-size: 0.75rem; font-weight: 600; text-transform: uppercase;
      letter-spacing: 0.05em; color: var(--muted); margin-bottom: 6px;
    }
    .kpi-value { font-size: 1.75rem; font-weight: 800; line-height: 1.2; }
    .kpi-sub { font-size: 0.8rem; color: var(--muted); margin-top: 4px; }
    .badge {
      display: inline-flex; align-items: center; padding: 3px 10px;
      border-radius: 999px; font-size: 0.7rem; font-weight: 700;
      letter-spacing: 0.04em; text-transform: uppercase;
    }
    .tier-Elite    { background: var(--green-soft); color: var(--green); }
    .tier-Strong   { background: var(--blue-soft);  color: var(--blue); }
    .tier-Stable   { background: var(--amber-soft); color: var(--amber); }
    .tier-Watch    { background: #fff7ed;           color: #ea580c; }
    .tier-Critical { background: var(--red-soft);   color: var(--red); }
    .impact-critical { background: var(--red-soft);   color: var(--red); }
    .impact-serious  { background: #fff7ed;           color: #ea580c; }
    .impact-moderate { background: var(--amber-soft); color: var(--amber); }
    .impact-minor    { background: var(--blue-soft);  color: var(--blue); }
    .grid-2 { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 16px; }
    .grid-3 { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 16px; }
    .panel {
      background: var(--surface); border: 1px solid var(--border);
      border-radius: var(--radius); padding: 20px; box-shadow: var(--shadow);
    }
    .panel h3 { font-size: 0.9rem; font-weight: 700; color: var(--ink); margin-bottom: 12px; }
    .panel .help-text { font-size: 0.78rem; color: var(--muted); margin-bottom: 10px; line-height: 1.5; }
    .chart { width: 100%; height: 360px; }
    .chart-tall { width: 100%; height: 400px; }
    .table-wrap {
      background: var(--surface); border: 1px solid var(--border);
      border-radius: var(--radius); box-shadow: var(--shadow); overflow-x: auto; margin-bottom: 16px;
    }
    table { width: 100%; border-collapse: collapse; font-size: 0.85rem; }
    th, td { padding: 10px 14px; text-align: left; border-bottom: 1px solid var(--border); }
    th {
      font-size: 0.7rem; font-weight: 700; text-transform: uppercase;
      letter-spacing: 0.05em; color: var(--muted); background: #f9fafb;
      position: sticky; top: 0; z-index: 1;
    }
    tr:hover td { background: #f9fafb; }
    td.num { text-align: right; font-variant-numeric: tabular-nums; }
    .violation-list { list-style: none; }
    .violation-item {
      display: flex; align-items: center; gap: 10px;
      padding: 10px 14px; border-bottom: 1px solid var(--border);
    }
    .violation-item:last-child { border-bottom: none; }
    .violation-count { font-size: 1.1rem; font-weight: 800; min-width: 36px; text-align: center; }
    .violation-info { flex: 1; }
    .violation-info .id { font-weight: 700; font-size: 0.85rem; }
    .violation-info .desc { font-size: 0.8rem; color: var(--muted); }
    .glossary-grid {
      display: grid; grid-template-columns: repeat(auto-fill, minmax(260px, 1fr)); gap: 12px;
    }
    .term {
      background: var(--surface); border: 1px solid var(--border);
      border-radius: var(--radius-sm); padding: 14px;
    }
    .term h4 { font-size: 0.85rem; font-weight: 700; color: var(--accent); margin-bottom: 4px; }
    .term p { font-size: 0.8rem; color: var(--muted); line-height: 1.5; }
    @media (max-width: 1100px) { .grid-2, .grid-3 { grid-template-columns: 1fr; } }
    @media (max-width: 680px) { .container { padding: 12px; } .kpi-grid { grid-template-columns: repeat(2, 1fr); } }
    @media print {
      body { background: #fff; }
      .header { background: #1e1b4b !important; -webkit-print-color-adjust: exact; }
      .kpi, .panel, .table-wrap { break-inside: avoid; }
    }
  </style>
</head>
<body>
<div class="container">

  <header class="header" role="banner">
    <h1>ðŸ“Š Performance &amp; Accessibility Benchmark Report</h1>
    <div class="meta">
      <span>Run ID: <strong>\${payload.runId}</strong></span>
      <span>Generated: <strong>\${new Date(payload.generatedAt).toLocaleString()}</strong></span>
      <span>Tests: <strong>\${payload.overall.totalTests}</strong></span>
      <span>Browsers: <strong>\${payload.browserRows.map((r) => r.browser).join(', ')}</strong></span>
    </div>
  </header>

  <h2 class="section-title"><span class="icon">ðŸŽ¯</span> Overall Health at a Glance</h2>
  <p style="color:var(--muted);font-size:0.85rem;margin-bottom:14px;">
    These cards summarize the entire test run. Green numbers are good, amber needs attention, red needs action.
  </p>
  <div class="kpi-grid">
    <article class="kpi">
      <div class="kpi-label">Benchmark Score</div>
      <div class="kpi-value">\${payload.overall.benchmarkScore}<small>/100</small></div>
      <span class="badge tier-\${payload.overall.benchmarkTier}">\${payload.overall.benchmarkTier}</span>
      <div class="kpi-sub">Weighted score combining speed, reliability, quality &amp; accessibility</div>
    </article>
    <article class="kpi">
      <div class="kpi-label">Pass Rate</div>
      <div class="kpi-value" style="color:\${payload.overall.passRatePct >= 90 ? 'var(--green)' : payload.overall.passRatePct >= 70 ? 'var(--amber)' : 'var(--red)'}">\${payload.overall.passRatePct}%</div>
      <div class="kpi-sub">How many tests finished successfully</div>
    </article>
    <!-- ... remaining KPI cards are generated dynamically ... -->
  </div>

  <!-- Full HTML is generated by the buildHtml function at runtime -->
  <!-- See the complete source above for the full template -->

</div>
<script>
  // Charts and tables are populated via Plotly.js at runtime
  // The full script block renders 3D charts, radar, box plots, tables, etc.
</script>
</body>
</html>\`;
}

// ---------------------------------------------------------------------------
//  Main
// ---------------------------------------------------------------------------

async function main(): Promise<void> {
  const metricsPath = process.argv[2] ? path.resolve(process.argv[2]) : DEFAULT_INPUT;
  const raw = await fs.readFile(metricsPath, 'utf-8');
  const summary = JSON.parse(raw) as ObservabilitySummary;
  const payload = buildPayload(summary);

  await fs.mkdir(OUTPUT_DIR, { recursive: true });
  await fs.writeFile(OUTPUT_FILE, buildHtml(payload), 'utf-8');

  console.log('[benchmark] Report written to ' + OUTPUT_FILE);
}

main().catch((error) => {
  const message = error instanceof Error ? error.message : String(error);
  console.error('[benchmark] Failed to build report: ' + message);
  process.exit(1);
});
```

> **âš ï¸ IMPORTANT NOTE:** The `buildHtml` function above is abbreviated in this blueprint for readability. The full version is in the companion file **`BENCHMARK-REPORT-GENERATOR-FULL.md`** included alongside this blueprint. That file contains the complete ~960-line source that you paste directly into `scripts/generate-performance-benchmark-report.ts`. All other files in this blueprint are 100% complete.

---

### ðŸ“„ `README.md`

```markdown
# Playwright POM Framework

A Playwright + TypeScript test framework with:
- **Page Object Model (POM)** â€” selectors & actions live in `pages/`, tests live in `tests/`
- **5 core test cases** for the VS Code getting-started docs page
- **Observability metrics** â€” network requests, errors, response times captured automatically
- **Accessibility scanning** â€” every page is scanned for WCAG violations (missing alt text, empty buttons, heading order, etc.)
- **3 reports generated in one command** â€” all output goes to `Reports/`

## Setup
```bash
npm install
npx playwright install
```

## Run Everything (One Command)
```bash
npm run reports
```

This single command:
1. Cleans old report artifacts
2. Runs all Playwright tests (Chromium + Firefox)
3. Generates the **Observability & Performance Benchmark Report** (with accessibility data)
4. Generates the **Allure HTML Report**
5. Keeps the **Playwright HTML Report** ready

## Reports Location
All output lives under `Reports/`:

```text
Reports/
â”œâ”€â”€ playwright-html/          â† Playwright's built-in HTML report
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ allure-report/            â† Allure HTML report
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ observability-metrics.json    â† Raw JSON metrics
â”‚   â””â”€â”€ performance-benchmark-report.html  â† Performance + Accessibility dashboard
â”œâ”€â”€ allure-results/           â† Raw Allure result files
â””â”€â”€ test-results/             â† Failure screenshots, videos, traces
```

## Open Reports
**Playwright HTML:**
```bash
npm run report:playwright
```

**Allure HTML:**
```bash
npm run report:allure:open
```

**Performance + Accessibility Dashboard:**
Open `Reports/observability/performance-benchmark-report.html` in any browser.

## What's in the Performance + Accessibility Report?

### Performance Section
| Metric | What it means |
|--------|---------------|
| **Benchmark Score** | Combined score (0â€“100) from speed, reliability, quality, throughput & accessibility |
| **Benchmark Tier** | Grade: Elite (90+), Strong (75+), Stable (60+), Watch (40+), Critical (<40) |
| **Pass Rate** | % of tests that passed |
| **Throughput** | Tests completed per minute |
| **Median / P95 / P99** | Duration percentiles â€” P99 = worst-case slow test |
| **CV%** | Consistency metric â€” lower = more predictable run times |
| **Request Failure Rate** | Failed network requests / total requests |
| **Error Signals** | Combined count of request + HTTP + console + page errors |

### Accessibility Section
| Metric | What it means |
|--------|---------------|
| **Accessibility Score** | 0â€“100 score based on violations found (fewer = better) |
| **Violations by Impact** | Breakdown into Critical, Serious, Moderate, Minor |
| **Top Violations** | Most frequent issues (e.g., `image-alt`, `button-name`, `heading-order`) |
| **Pages with Issues** | How many test pages had at least one violation |

### Charts
- **3D Test Benchmark Cloud** â€” each dot = one test (size = error count)
- **3D Browser Comparison** â€” bubble size = benchmark score
- **Radar Chart** â€” 5 dimensions: Speed, Reliability, Quality, Throughput, Accessibility
- **Duration Box Plot** â€” min/median/max spread per browser
- **Tier Distribution** â€” pie chart of quality tiers
- **Throughput vs Pass Rate** â€” bar + line combo
- **Top 10 Slowest Tests** â€” bottleneck candidates

## Framework Structure
```text
pages/          â†’ Page Object classes (selectors + actions + assertions)
fixtures/       â†’ Fixture wiring + observability hooks + accessibility scanning
tests/          â†’ Test specs (scenario-only, no raw selectors)
reporters/      â†’ Custom observability reporter
scripts/        â†’ Benchmark report generator
observability/  â†’ TypeScript types for metrics
```

## Beginner Docs
- `AGENTS.md` â€” simple rules for writing tests
- `walkthrough.md` â€” step-by-step guide to understanding the framework
```

---

### ðŸ“„ `walkthrough.md`

```markdown
# Walkthrough

A step-by-step guide to understanding and using this framework. Written for beginners.

## 1) What this framework gives you
- **POM-based** test structure (selectors in `pages/`, logic in `tests/`)
- **Automatic observability** â€” every test captures network metrics, errors and timing
- **Automatic accessibility scanning** â€” every page is checked for WCAG violations
- **3 report outputs** from a single command:
  1. Playwright HTML report (built-in)
  2. Allure report (detailed history & categories)
  3. Performance + Accessibility benchmark dashboard (3D charts, KPI cards, violation list)

## 2) Get started
Install once:
```bash
npm install
npx playwright install
```

Run everything with **one command**:
```bash
npm run reports
```

This command does: clean â†’ test â†’ generate benchmark report â†’ generate Allure report.

## 3) Where to see results
| Report | Location |
|--------|----------|
| Playwright HTML | `Reports/playwright-html/index.html` |
| Allure HTML | `Reports/allure-report/index.html` |
| Benchmark Dashboard | `Reports/observability/performance-benchmark-report.html` |
| Raw metrics JSON | `Reports/observability/observability-metrics.json` |
| Failure traces/screenshots/videos | `Reports/test-results/` |

## 4) File reading order (recommended)
Read these files in this order to understand the framework:

1. **`package.json`** â€” npm scripts (how to run things)
2. **`playwright.config.ts`** â€” browsers, reporters, timeouts
3. **`observability/types.ts`** â€” data shapes (metrics + accessibility types)
4. **`fixtures/observability.fixture.ts`** â€” auto-captures network, errors, accessibility per test
5. **`fixtures/test.fixture.ts`** â€” injects page objects into tests
6. **`pages/GettingStartedVscodePage.ts`** â€” selectors + reusable methods
7. **`tests/getting-started-vscode.spec.ts`** â€” 5 test scenarios
8. **`reporters/observability-reporter.ts`** â€” aggregates metrics into JSON
9. **`scripts/generate-performance-benchmark-report.ts`** â€” builds the HTML dashboard

## 5) How to add a new test
1. Add a new method in the page object (`pages/...Page.ts`) first.
2. Call that method from a spec file (`tests/...spec.ts`).
3. **Never** use raw selectors in spec files â€” always go through the page object.
4. Run:
   ```bash
   npm run reports
   ```
5. Check all three reports under `Reports/`.

## 6) Understanding the benchmark report

### KPI Cards (top section)
- **Benchmark Score** â€” overall health (0â€“100). Combines speed + reliability + quality + throughput + accessibility.
- **Pass Rate** â€” what % of tests passed. Green = good, red = bad.
- **Median Duration** â€” typical test time. Lower is better.
- **P99 Duration** â€” worst-case test time (only 1% are slower).
- **CV%** â€” consistency. Low = predictable. High = flaky timing.
- **Throughput** â€” tests per minute.

### Accessibility Section
- **Accessibility Score** â€” 0â€“100. Fewer violations = higher score.
- **Violations by Impact** â€” Critical > Serious > Moderate > Minor.
- **Top Violations** â€” most common issues like missing alt text, empty buttons, heading order.
- Fix **critical** and **serious** issues first.

### Charts
- **3D Test Cloud** â€” drag to rotate, scroll to zoom. Each dot is a test.
- **Radar Chart** â€” compares browsers across 5 dimensions.
- **Box Plot** â€” shows duration spread per browser.
- **Tier Pie** â€” how many tests are Elite/Strong/Stable/Watch/Critical.

## 7) Debug checklist
1. Open the Playwright HTML report first â€” it shows pass/fail clearly.
2. Look at traces/videos in `Reports/test-results/` for failing tests.
3. Check the benchmark dashboard for performance regressions or new accessibility violations.
4. Validate selectors in the page object if a test can't find an element.
5. Re-run with `npm run reports` after fixing.
```

---

### ðŸ“„ `AGENTS.md`

```markdown
# AGENTS.md

This guide is for juniors and beginners writing tests in this framework.

## Read first
1. `walkthrough.md` â€” how the framework works step by step
2. `README.md` â€” setup, commands, report explanations

## Main rules
1. Follow **Page Object Model** strictly â€” all selectors in `pages/`, all test logic in `tests/`.
2. Keep selectors in `pages/` only â€” never use raw locators in spec files.
3. Keep test logic in `tests/` only â€” page objects handle actions + assertions.
4. Keep each test focused on **one behavior** (one clear thing to verify).
5. Use clear, descriptive test names.
6. Accessibility is checked automatically â€” you don't need to add a11y assertions manually.

## Folder responsibility
| Folder / File | Purpose |
|---------------|---------|
| `tests/` | Test scenarios only (import fixtures, call page object methods) |
| `pages/` | Locators + actions + assertions (Page Object classes) |
| `fixtures/test.fixture.ts` | Injects page objects into tests |
| `fixtures/observability.fixture.ts` | Auto-captures network metrics, errors & accessibility per test |
| `observability/types.ts` | TypeScript types for metrics & accessibility data |
| `reporters/observability-reporter.ts` | Aggregates metrics into JSON output |
| `scripts/generate-performance-benchmark-report.ts` | Builds the HTML benchmark + accessibility dashboard |

## Run and report (single command)
Use **one command** for full execution + all 3 reports:
```bash
npm run reports
```

This runs: clean â†’ tests â†’ benchmark report â†’ Allure report.

All outputs are available in `Reports/`:
| Report | Path |
|--------|------|
| Playwright HTML | `Reports/playwright-html/index.html` |
| Allure HTML | `Reports/allure-report/index.html` |
| Benchmark + Accessibility Dashboard | `Reports/observability/performance-benchmark-report.html` |
| Raw metrics JSON | `Reports/observability/observability-metrics.json` |
| Failure artifacts | `Reports/test-results/` |

## Test writing template
```ts
import { test } from '../fixtures/test.fixture';

test.describe('Feature name', () => {
  test.beforeEach(async ({ docsPage }) => {
    await docsPage.open();
  });

  test('does one important check', async ({ docsPage }) => {
    await docsPage.assertTitleAndMainHeading();
  });
});
```

## What gets captured automatically (no code needed)
Every test automatically collects:
- âœ… Network request count, failures, response times
- âœ… Console errors and page crashes
- âœ… Test duration and timing
- âœ… **Accessibility scan** â€” checks for missing alt text, empty buttons, heading order, form labels, missing landmarks, and more

All of this flows into the benchmark report automatically.

## Definition of done
- [ ] Test passes locally (`npm run reports`)
- [ ] Test uses POM (no raw selectors in the spec file)
- [ ] `npm run reports` completes and generates all 3 reports in `Reports/`
- [ ] No new critical/serious accessibility violations introduced
```

---

## Step 3 â€” Install Dependencies

```bash
npm install
npx playwright install
```

This installs:
- `@playwright/test` â€” the test framework
- `allure-playwright` + `allure-commandline` â€” Allure reporting
- `tsx` â€” TypeScript execution for the benchmark script
- `typescript` + `@types/node` â€” TypeScript support
- Chromium + Firefox browsers (via `npx playwright install`)

---

## Step 4 â€” Run Everything

One command to run all tests and generate all 3 reports:

```bash
npm run reports
```

Expected output:
```
Running 10 tests using 1 worker

  âœ“  [chromium] tests/getting-started-vscode.spec.ts:5:3 â€º ...
  âœ“  [chromium] tests/getting-started-vscode.spec.ts:9:3 â€º ...
  ... (10 tests total â€” 5 per browser)

  10 passed

[observability] Metrics written to Reports/observability/observability-metrics.json
[benchmark] Report written to Reports/observability/performance-benchmark-report.html
Report successfully generated to Reports/allure-report
```

---

## Step 5 â€” View Reports

### Playwright HTML Report
```bash
npm run report:playwright
```
Opens at `http://localhost:9323` â€” shows pass/fail, screenshots, videos, traces.

### Allure HTML Report
```bash
npm run report:allure:open
```
Opens via HTTP server â€” shows detailed test history, categories, attachments.

> **Note:** Allure reports require an HTTP server. Don't double-click the `index.html` â€” it won't work over `file://` protocol.

### Performance + Accessibility Dashboard
Open directly in any browser:
```bash
open Reports/observability/performance-benchmark-report.html
```
Features: 3D interactive charts (Plotly.js), KPI cards, accessibility violation list, browser comparison table, per-test data table, glossary.

### Open All Reports at Once
```bash
npm run report:open:all
```

---

## Project Structure Reference

```
playwright-observability-framework/
â”œâ”€â”€ package.json                          â† Dependencies & npm scripts
â”œâ”€â”€ tsconfig.json                         â† TypeScript config
â”œâ”€â”€ playwright.config.ts                  â† Playwright config (browsers, reporters, capture settings)
â”œâ”€â”€ README.md                             â† Project overview & setup
â”œâ”€â”€ walkthrough.md                        â† Step-by-step beginner guide
â”œâ”€â”€ AGENTS.md                             â† Rules for writing tests (for AI agents & juniors)
â”‚
â”œâ”€â”€ observability/
â”‚   â””â”€â”€ types.ts                          â† TypeScript interfaces for all metrics
â”‚
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ observability.fixture.ts          â† Auto-captures network, errors, accessibility per test
â”‚   â””â”€â”€ test.fixture.ts                   â† Injects page objects into tests
â”‚
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ GettingStartedVscodePage.ts       â† Page Object (selectors + actions + assertions)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ getting-started-vscode.spec.ts    â† 5 test scenarios using POM
â”‚
â”œâ”€â”€ reporters/
â”‚   â””â”€â”€ observability-reporter.ts         â† Aggregates per-test metrics into JSON
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ allure-cli.js                     â† Allure wrapper (fixes paths with spaces)
â”‚   â””â”€â”€ generate-performance-benchmark-report.ts  â† Builds HTML dashboard (~960 lines)
â”‚
â””â”€â”€ Reports/                              â† Generated after running tests (gitignored)
    â”œâ”€â”€ playwright-html/index.html
    â”œâ”€â”€ allure-report/index.html
    â”œâ”€â”€ observability/
    â”‚   â”œâ”€â”€ observability-metrics.json
    â”‚   â””â”€â”€ performance-benchmark-report.html
    â”œâ”€â”€ allure-results/
    â””â”€â”€ test-results/
```

---

## How to Add New Tests

1. **Create a page object** in `pages/` (or add methods to an existing one):
   ```typescript
   // pages/MyNewPage.ts
   import { expect, Locator, Page } from '@playwright/test';

   export class MyNewPage {
     private readonly page: Page;
     readonly heading: Locator;

     constructor(page: Page) {
       this.page = page;
       this.heading = page.getByRole('heading', { level: 1 });
     }

     async open(): Promise<void> {
       await this.page.goto('/my-page');
     }

     async assertHeadingVisible(): Promise<void> {
       await expect(this.heading).toBeVisible();
     }
   }
   ```

2. **Register it in the fixture** (`fixtures/test.fixture.ts`):
   ```typescript
   import { MyNewPage } from '../pages/MyNewPage';

   type AppFixtures = {
     docsPage: GettingStartedVscodePage;
     myNewPage: MyNewPage;  // â† add here
   };

   export const test = base.extend<AppFixtures>({
     docsPage: async ({ page }, use) => { await use(new GettingStartedVscodePage(page)); },
     myNewPage: async ({ page }, use) => { await use(new MyNewPage(page)); },  // â† add here
   });
   ```

3. **Write the test** in `tests/`:
   ```typescript
   import { test } from '../fixtures/test.fixture';

   test.describe('My new feature', () => {
     test('heading is visible', async ({ myNewPage }) => {
       await myNewPage.open();
       await myNewPage.assertHeadingVisible();
     });
   });
   ```

4. **Run and verify:**
   ```bash
   npm run reports
   ```

---

## Troubleshooting

| Problem | Fix |
|---------|-----|
| `allure: command not found` | Use `npm run report:allure:generate` instead of raw `allure` â€” the wrapper handles it |
| Allure report blank when opened as file | Allure needs HTTP server â€” use `npm run report:allure:open` |
| Path with spaces breaks allure | The `scripts/allure-cli.js` wrapper handles this automatically |
| No screenshots/videos in report | Check `playwright.config.ts` â€” `screenshot` and `video` should be `'on'` |
| `npx playwright install` fails | Make sure Node.js 18+ is installed, try `npx playwright install --with-deps` |
| Java not found (Allure) | Install Java 8+: `brew install openjdk` (macOS) or `apt install default-jdk` (Ubuntu) |
| Tests timeout | Increase `timeout` in `playwright.config.ts` or check network connectivity |

---

> **That's it!** Create the folder, paste each file, run `npm install && npx playwright install && npm run reports`, and you'll have the full framework with all 3 reports.
